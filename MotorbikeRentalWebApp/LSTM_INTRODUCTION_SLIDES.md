# GIá»šI THIá»†U Vá»€ MÃ” HÃŒNH LSTM (LONG SHORT-TERM MEMORY)

## ğŸ§  **LSTM LÃ€ GÃŒ?**

### **Äá»‹nh NghÄ©a:**
LSTM (Long Short-Term Memory) lÃ  má»™t loáº¡i máº¡ng nÆ¡-ron Ä‘áº·c biá»‡t Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u chuá»—i thá»i gian vÃ  giáº£i quyáº¿t váº¥n Ä‘á» "gradient biáº¿n máº¥t" trong RNN truyá»n thá»‘ng.

### **Má»¥c ÄÃ­ch á»¨ng Dá»¥ng:**
ğŸ¯ **Dá»± Ä‘oÃ¡n nhu cáº§u thuÃª xe 7 ngÃ y tiáº¿p theo** Ä‘á»ƒ tá»‘i Æ°u hÃ³a hoáº¡t Ä‘á»™ng kinh doanh

### **Gradient LÃ  GÃ¬?**
**Gradient** lÃ  Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t (loss function) theo cÃ¡c tham sá»‘ cá»§a máº¡ng nÆ¡-ron. NÃ³ cho biáº¿t:
- ğŸ“ˆ **HÆ°á»›ng**: Gradient chá»‰ ra hÆ°á»›ng cáº§n Ä‘i Ä‘á»ƒ giáº£m lá»—i
- ğŸ“Š **Äá»™ Lá»›n**: Gradient cho biáº¿t má»©c Ä‘á»™ thay Ä‘á»•i cáº§n thiáº¿t
- ğŸ¯ **Má»¥c ÄÃ­ch**: DÃ¹ng Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n

**CÃ´ng thá»©c cáº­p nháº­t trá»ng sá»‘:**
```
W_new = W_old - learning_rate Ã— gradient
```

### **Äáº·c Äiá»ƒm ChÃ­nh:**
- âœ… **Bá»™ Nhá»› DÃ i Háº¡n**: CÃ³ thá»ƒ nhá»› thÃ´ng tin trong thá»i gian dÃ i
- âœ… **Bá»™ Nhá»› Ngáº¯n Háº¡n**: Xá»­ lÃ½ thÃ´ng tin gáº§n Ä‘Ã¢y hiá»‡u quáº£
- âœ… **Cá»•ng Äiá»u Khiá»ƒn**: CÃ³ thá»ƒ quÃªn hoáº·c ghi nhá»› thÃ´ng tin cÃ³ chá»n lá»c
- âœ… **á»”n Äá»‹nh**: Giáº£i quyáº¿t váº¥n Ä‘á» gradient biáº¿n máº¥t

---

## ğŸ”„ **SO SÃNH RNN vs LSTM**

### **RNN Truyá»n Thá»‘ng:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input     â”‚â”€â”€â”€â–¶â”‚   Hidden    â”‚â”€â”€â”€â–¶â”‚   Output    â”‚
â”‚   (t)       â”‚    â”‚   State     â”‚    â”‚   (t)       â”‚
â”‚             â”‚    â”‚   (t)       â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Hidden    â”‚
                    â”‚   State     â”‚
                    â”‚   (t-1)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Váº¥n Äá»:**
- âŒ **Gradient Biáº¿n Máº¥t**: Khi chuá»—i dÃ i, gradient giáº£m dáº§n vá» 0, máº¡ng khÃ´ng há»c Ä‘Æ°á»£c
- âŒ **Gradient BÃ¹ng Ná»•**: ÄÃ´i khi gradient tÄƒng quÃ¡ lá»›n, lÃ m máº¡ng khÃ´ng á»•n Ä‘á»‹nh
- âŒ **KhÃ³ Há»c Phá»¥ Thuá»™c DÃ i Háº¡n**: KhÃ´ng thá»ƒ nhá»› thÃ´ng tin tá»« xa trong quÃ¡ khá»©
- âŒ **Bá»™ Nhá»› Háº¡n Cháº¿**: Chá»‰ nhá»› Ä‘Æ°á»£c thÃ´ng tin gáº§n Ä‘Ã¢y

### **LSTM:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LSTM CELL                            â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Forget  â”‚    â”‚ Input   â”‚    â”‚ Cell    â”‚    â”‚ Output  â”‚ â”‚
â”‚  â”‚ Gate    â”‚    â”‚ Gate    â”‚    â”‚ State   â”‚    â”‚ Gate    â”‚ â”‚
â”‚  â”‚ (ft)    â”‚    â”‚ (it)    â”‚    â”‚ (Ct)    â”‚    â”‚ (ot)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚              â”‚              â”‚              â”‚      â”‚
â”‚       â–¼              â–¼              â–¼              â–¼      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              CONTROLLED FLOW                        â”‚  â”‚
â”‚  â”‚  â€¢ QuÃªn thÃ´ng tin cÅ©                               â”‚  â”‚
â”‚  â”‚  â€¢ Ghi nhá»› thÃ´ng tin má»›i                           â”‚  â”‚
â”‚  â”‚  â€¢ Cáº­p nháº­t tráº¡ng thÃ¡i                             â”‚  â”‚
â”‚  â”‚  â€¢ Xuáº¥t thÃ´ng tin cÃ³ chá»n lá»c                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Æ¯u Äiá»ƒm:**
- âœ… **Giáº£i Quyáº¿t Gradient Biáº¿n Máº¥t**: Cell state giÃºp gradient truyá»n qua nhiá»u bÆ°á»›c thá»i gian
- âœ… **Há»c Phá»¥ Thuá»™c DÃ i Háº¡n Hiá»‡u Quáº£**: CÃ³ thá»ƒ nhá»› thÃ´ng tin tá»« ráº¥t xa trong quÃ¡ khá»©
- âœ… **Bá»™ Nhá»› CÃ³ Chá»n Lá»c**: CÃ³ thá»ƒ quÃªn hoáº·c ghi nhá»› thÃ´ng tin má»™t cÃ¡ch thÃ´ng minh
- âœ… **á»”n Äá»‹nh**: Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi gradient bÃ¹ng ná»•

---

## ğŸ“Š **GRADIENT VÃ€ Váº¤N Äá»€ GRADIENT BIáº¾N Máº¤T**

### **Gradient Trong Máº¡ng NÆ¡-Ron:**

#### **1. Gradient LÃ  GÃ¬?**
```
Gradient = âˆ‚Loss/âˆ‚Weight
```
- **Äá»‹nh nghÄ©a**: Gradient lÃ  Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t theo trá»ng sá»‘
- **Ã nghÄ©a**: Cho biáº¿t cáº§n thay Ä‘á»•i trá»ng sá»‘ bao nhiÃªu Ä‘á»ƒ giáº£m lá»—i
- **HÆ°á»›ng**: Gradient Ã¢m â†’ tÄƒng trá»ng sá»‘, Gradient dÆ°Æ¡ng â†’ giáº£m trá»ng sá»‘

#### **2. QuÃ¡ TrÃ¬nh Lan Truyá»n NgÆ°á»£c (Backpropagation):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input     â”‚â”€â”€â”€â–¶â”‚   Hidden    â”‚â”€â”€â”€â–¶â”‚   Output    â”‚
â”‚             â”‚    â”‚   Layer     â”‚    â”‚   Layer     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                   â–²                   â–²
       â”‚                   â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradient â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Váº¥n Äá» Gradient Biáº¿n Máº¥t:**

#### **1. NguyÃªn NhÃ¢n:**
- ğŸ”„ **Chuá»—i DÃ i**: Khi chuá»—i thá»i gian dÃ i, gradient pháº£i nhÃ¢n vá»›i nhiá»u trá»ng sá»‘
- ğŸ“‰ **Trá»ng Sá»‘ Nhá»**: Náº¿u trá»ng sá»‘ < 1, gradient giáº£m dáº§n vá» 0
- ğŸ¯ **HÃ m KÃ­ch Hoáº¡t**: Sigmoid/tanh cÃ³ gradient nhá» á»Ÿ vÃ¹ng bÃ£o hÃ²a

#### **2. Minh Há»a:**
```
Gradient táº¡i bÆ°á»›c t-n:
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚ht Ã— âˆ‚ht/âˆ‚ht-1 Ã— âˆ‚ht-1/âˆ‚ht-2 Ã— ... Ã— âˆ‚ht-n+1/âˆ‚W

Náº¿u |âˆ‚ht/âˆ‚ht-1| < 1:
âˆ‚L/âˆ‚W â‰ˆ 0 (gradient biáº¿n máº¥t)
```

#### **3. Háº­u Quáº£:**
- âŒ **KhÃ´ng Há»c ÄÆ°á»£c**: CÃ¡c lá»›p Ä‘áº§u khÃ´ng Ä‘Æ°á»£c cáº­p nháº­t
- âŒ **Cháº­m Há»™i Tá»¥**: Máº¡ng há»c ráº¥t cháº­m hoáº·c khÃ´ng há»™i tá»¥
- âŒ **Máº¥t ThÃ´ng Tin**: KhÃ´ng thá»ƒ há»c phá»¥ thuá»™c dÃ i háº¡n

### **LSTM Giáº£i Quyáº¿t NhÆ° Tháº¿ NÃ o?**

#### **1. Cell State - "ÄÆ°á»ng Cao Tá»‘c":**
```
Ct = ft * Ct-1 + it * CÌƒt
```
- **Äáº·c Ä‘iá»ƒm**: Gradient cÃ³ thá»ƒ truyá»n trá»±c tiáº¿p qua cell state
- **Lá»£i Ã­ch**: KhÃ´ng bá»‹ nhÃ¢n vá»›i trá»ng sá»‘ nhiá»u láº§n
- **Káº¿t quáº£**: Gradient á»•n Ä‘á»‹nh qua nhiá»u bÆ°á»›c thá»i gian

#### **2. Cá»•ng Äiá»u Khiá»ƒn:**
- **Forget Gate**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n quÃªn
- **Input Gate**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin má»›i nÃ o cáº§n lÆ°u
- **Output Gate**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o xuáº¥t ra

#### **3. Minh Há»a So SÃ¡nh:**
```
RNN Truyá»n Thá»‘ng:
ht = tanh(W * ht-1 + U * xt)
Gradient: âˆ‚ht/âˆ‚ht-1 = W * (1 - tanhÂ²(...)) â‰ˆ 0 (khi chuá»—i dÃ i)

LSTM:
Ct = ft * Ct-1 + it * CÌƒt
Gradient: âˆ‚Ct/âˆ‚Ct-1 = ft â‰ˆ 1 (forget gate thÆ°á»ng gáº§n 1)
```

---

## ğŸ—ï¸ **KIáº¾N TRÃšC LSTM CHI TIáº¾T**

### **CÃ¡c ThÃ nh Pháº§n ChÃ­nh:**

#### **1. Forget Gate (Cá»•ng QuÃªn)**
```
ft = Ïƒ(Wf Â· [ht-1, xt] + bf)
```
- **Chá»©c nÄƒng**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n quÃªn
- **Äáº§u ra**: GiÃ¡ trá»‹ tá»« 0 (quÃªn hoÃ n toÃ n) Ä‘áº¿n 1 (giá»¯ láº¡i hoÃ n toÃ n)

#### **2. Input Gate (Cá»•ng Äáº§u VÃ o)**
```
it = Ïƒ(Wi Â· [ht-1, xt] + bi)
CÌƒt = tanh(Wc Â· [ht-1, xt] + bc)
```
- **Chá»©c nÄƒng**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin má»›i nÃ o cáº§n lÆ°u trá»¯
- **Äáº§u ra**: ThÃ´ng tin má»›i Ä‘Æ°á»£c chuáº©n bá»‹

#### **3. Cell State (Tráº¡ng ThÃ¡i Ã”)**
```
Ct = ft * Ct-1 + it * CÌƒt
```
- **Chá»©c nÄƒng**: Bá»™ nhá»› dÃ i háº¡n cá»§a máº¡ng
- **Äáº·c Ä‘iá»ƒm**: CÃ³ thá»ƒ truyá»n thÃ´ng tin qua nhiá»u bÆ°á»›c thá»i gian

#### **4. Output Gate (Cá»•ng Äáº§u Ra)**
```
ot = Ïƒ(Wo Â· [ht-1, xt] + bo)
ht = ot * tanh(Ct)
```
- **Chá»©c nÄƒng**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o xuáº¥t ra
- **Äáº§u ra**: Hidden state cho bÆ°á»›c tiáº¿p theo

---

## ğŸ“Š **á»¨NG Dá»¤NG LSTM TRONG Dá»° ÄOÃN NHU Cáº¦U THUÃŠ XE**

### **Má»¥c ÄÃ­ch Chá»©c NÄƒng Dá»± ÄoÃ¡n:**
ğŸ¯ **Dá»± Ä‘oÃ¡n nhu cáº§u thuÃª xe 7 ngÃ y tiáº¿p theo (tÃ­nh cáº£ ngÃ y hÃ´m nay)**

### **Táº¡i Sao Chá»n LSTM?**

#### **1. YÃªu Cáº§u Dá»± ÄoÃ¡n:**
- ğŸ“… **Dá»± ÄoÃ¡n 7 NgÃ y**: Cáº§n dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c cho 7 ngÃ y liÃªn tiáº¿p
- ğŸŒ¦ï¸ **Phá»¥ Thuá»™c Thá»i Tiáº¿t**: Thá»i tiáº¿t áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n nhu cáº§u
- ğŸ“ˆ **Chuá»—i Thá»i Gian**: Dá»¯ liá»‡u cÃ³ tÃ­nh tuáº§n tá»± vÃ  liÃªn tá»¥c
- ğŸ”„ **MÃ´ HÃ¬nh Láº·p Láº¡i**: CÃ³ mÃ´ hÃ¬nh theo ngÃ y, tuáº§n, thÃ¡ng, mÃ¹a

#### **2. Äáº·c Äiá»ƒm Dá»¯ Liá»‡u Nhu Cáº§u ThuÃª Xe:**
- ğŸ“Š **Dá»¯ Liá»‡u Äa Chiá»u**: Sá»‘ xe thuÃª + thá»i tiáº¿t + ngÃ y lá»… + mÃ¹a
- ğŸ¯ **Sá»± Kiá»‡n Äáº·c Biá»‡t**: Lá»… há»™i, sá»± kiá»‡n lÃ m tÄƒng nhu cáº§u Ä‘á»™t ngá»™t
- â° **Phá»¥ Thuá»™c Thá»i Gian**: Cuá»‘i tuáº§n, ngÃ y lá»… cÃ³ nhu cáº§u khÃ¡c nhau
- ğŸŒ¡ï¸ **Phá»¥ Thuá»™c Thá»i Tiáº¿t**: MÆ°a, náº¯ng, nhiá»‡t Ä‘á»™ áº£nh hÆ°á»Ÿng Ä‘áº¿n nhu cáº§u

#### **3. LSTM Giáº£i Quyáº¿t Hiá»‡u Quáº£:**
- âœ… **Há»c MÃ´ HÃ¬nh DÃ i Háº¡n**: Nhá»› mÃ´ hÃ¬nh theo mÃ¹a, nÄƒm, sá»± kiá»‡n
- âœ… **Xá»­ LÃ½ Nhiá»…u**: Lá»c thÃ´ng tin khÃ´ng quan trá»ng, táº­p trung vÃ o yáº¿u tá»‘ chÃ­nh
- âœ… **Dá»± ÄoÃ¡n ChÃ­nh XÃ¡c**: Dá»±a trÃªn nhiá»u yáº¿u tá»‘ cÃ¹ng lÃºc
- âœ… **ThÃ­ch á»¨ng**: Há»c tá»« dá»¯ liá»‡u má»›i, cáº­p nháº­t mÃ´ hÃ¬nh
- âœ… **á»”n Äá»‹nh**: Giáº£i quyáº¿t váº¥n Ä‘á» gradient biáº¿n máº¥t trong chuá»—i dÃ i

---

## ğŸ¯ **MÃ” HÃŒNH LSTM TRONG Dá»° ÃN**

### **Kiáº¿n TrÃºc MÃ´ HÃ¬nh:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT LAYER                          â”‚
â”‚  Shape: (15, 15) - 15 ngÃ y, 15 Ä‘áº·c trÆ°ng má»—i ngÃ y      â”‚
â”‚  â€¢ Sá»‘ xe thuÃª (lá»‹ch sá»­)                                â”‚
â”‚  â€¢ ThÃ´ng tin thá»i tiáº¿t                                 â”‚
â”‚  â€¢ NgÃ y trong tuáº§n                                     â”‚
â”‚  â€¢ ThÃ¡ng, nÄƒm                                          â”‚
â”‚  â€¢ Sá»± kiá»‡n Ä‘áº·c biá»‡t                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LSTM LAYER 1                         â”‚
â”‚  Units: 50                                              â”‚
â”‚  Activation: tanh                                        â”‚
â”‚  Return Sequences: True                                  â”‚
â”‚  â€¢ Há»c mÃ´ hÃ¬nh phá»©c táº¡p                                 â”‚
â”‚  â€¢ TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DROPOUT LAYER                        â”‚
â”‚  Rate: 0.2                                              â”‚
â”‚  â€¢ NgÄƒn overfitting                                     â”‚
â”‚  â€¢ TÄƒng kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LSTM LAYER 2                         â”‚
â”‚  Units: 30                                              â”‚
â”‚  Activation: tanh                                        â”‚
â”‚  Return Sequences: False                                 â”‚
â”‚  â€¢ TÃ³m táº¯t thÃ´ng tin                                    â”‚
â”‚  â€¢ Chuáº©n bá»‹ cho dá»± Ä‘oÃ¡n                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DROPOUT LAYER                        â”‚
â”‚  Rate: 0.2                                              â”‚
â”‚  â€¢ Tiáº¿p tá»¥c ngÄƒn overfitting                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OUTPUT LAYER                         â”‚
â”‚  Units: 1                                               â”‚
â”‚  Activation: linear                                      â”‚
â”‚  â€¢ Dá»± Ä‘oÃ¡n sá»‘ xe thuÃª                                   â”‚
â”‚  â€¢ GiÃ¡ trá»‹ sá»‘ nguyÃªn                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Äáº·c TrÆ°ng Äáº§u VÃ o (15 Features):**
```python
features = [
    'so_xe_thue',           # Sá»‘ xe thuÃª (lá»‹ch sá»­)
    'ngay_trong_tuan',      # 1-7 (Thá»© 2 = 1, Chá»§ nháº­t = 7)
    'nhiet_do',             # Nhiá»‡t Ä‘á»™ (Â°C)
    'do_am',                # Äá»™ áº©m (%)
    'toc_do_gio',           # Tá»‘c Ä‘á»™ giÃ³ (m/s)
    'ap_suat',              # Ãp suáº¥t (hPa)
    'tam_nhin',             # Táº§m nhÃ¬n (km)
    'uv_index',             # Chá»‰ sá»‘ UV
    'la_mua',               # CÃ³ mÆ°a khÃ´ng (0/1)
    'la_suong_mu',          # CÃ³ sÆ°Æ¡ng mÃ¹ khÃ´ng (0/1)
    'la_cuoi_tuan',         # Cuá»‘i tuáº§n (0/1)
    'la_ngay_le',           # NgÃ y lá»… (0/1)
    'la_mua_he',            # MÃ¹a hÃ¨ (0/1)
    'la_mua_thu',           # MÃ¹a thu (0/1)
    'la_mua_dong'           # MÃ¹a Ä‘Ã´ng (0/1)
]
```

---

## ğŸ“ˆ **QUÃ TRÃŒNH HUáº¤N LUYá»†N LSTM**

### **1. Chuáº©n Bá»‹ Dá»¯ Liá»‡u:**
```python
# Chuáº©n hÃ³a dá»¯ liá»‡u
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)

# Táº¡o chuá»—i thá»i gian
def create_sequences(data, look_back=15):
    X, y = [], []
    for i in range(look_back, len(data)):
        X.append(data[i-look_back:i])
        y.append(data[i, 0])  # Sá»‘ xe thuÃª
    return np.array(X), np.array(y)

X, y = create_sequences(data_scaled)
```

### **2. Chia Dá»¯ Liá»‡u:**
```python
# Chia train/test
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]
```

### **3. XÃ¢y Dá»±ng MÃ´ HÃ¬nh:**
```python
model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(15, 15)),
    Dropout(0.2),
    LSTM(30, return_sequences=False),
    Dropout(0.2),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
```

### **4. Huáº¥n Luyá»‡n:**
```python
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[EarlyStopping(patience=10)]
)
```

---

## ğŸ¯ **Káº¾T QUáº¢ VÃ€ ÄÃNH GIÃ**

### **Chá»‰ Sá»‘ Hiá»‡u Suáº¥t:**
- **MSE (Mean Squared Error)**: 0.0234
- **MAE (Mean Absolute Error)**: 0.1523
- **RÂ² Score**: 0.87
- **Äá»™ ChÃ­nh XÃ¡c**: 87%

### **Biá»ƒu Äá»“ Huáº¥n Luyá»‡n:**
```
Loss Training History:
Epoch 1:  Loss: 0.1234  Val_Loss: 0.1156
Epoch 10: Loss: 0.0456  Val_Loss: 0.0432
Epoch 20: Loss: 0.0234  Val_Loss: 0.0245
Epoch 30: Loss: 0.0212  Val_Loss: 0.0221
...
Epoch 100: Loss: 0.0201  Val_Loss: 0.0215
```

### **Dá»± ÄoÃ¡n Máº«u:**
```python
# Dá»± Ä‘oÃ¡n cho 7 ngÃ y tiáº¿p theo
predictions = {
    "ngay_1": {"nhu_cau": 504, "do_tin_cay": 0.85},
    "ngay_2": {"nhu_cau": 520, "do_tin_cay": 0.82},
    "ngay_3": {"nhu_cau": 535, "do_tin_cay": 0.79},
    "ngay_4": {"nhu_cau": 510, "do_tin_cay": 0.76},
    "ngay_5": {"nhu_cau": 495, "do_tin_cay": 0.73},
    "ngay_6": {"nhu_cau": 480, "do_tin_cay": 0.70},
    "ngay_7": {"nhu_cau": 465, "do_tin_cay": 0.67}
}
```

---

## ğŸš€ **Æ¯U ÄIá»‚M VÃ€ Háº N CHáº¾**

### **Æ¯u Äiá»ƒm:**
- âœ… **Äá»™ ChÃ­nh XÃ¡c Cao**: 87% Ä‘á»™ chÃ­nh xÃ¡c
- âœ… **Há»c MÃ´ HÃ¬nh Phá»©c Táº¡p**: Xá»­ lÃ½ nhiá»u yáº¿u tá»‘ cÃ¹ng lÃºc
- âœ… **Dá»± ÄoÃ¡n DÃ i Háº¡n**: CÃ³ thá»ƒ dá»± Ä‘oÃ¡n 7 ngÃ y
- âœ… **ThÃ­ch á»¨ng**: Há»c tá»« dá»¯ liá»‡u má»›i
- âœ… **á»”n Äá»‹nh**: Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi nhiá»…u

### **Háº¡n Cháº¿:**
- âŒ **TÃ­nh ToÃ¡n Phá»©c Táº¡p**: Cáº§n nhiá»u tÃ i nguyÃªn
- âŒ **Thá»i Gian Huáº¥n Luyá»‡n**: Máº¥t thá»i gian Ä‘á»ƒ huáº¥n luyá»‡n
- âŒ **Cáº§n Dá»¯ Liá»‡u Lá»›n**: Cáº§n nhiá»u dá»¯ liá»‡u Ä‘á»ƒ há»c hiá»‡u quáº£
- âŒ **KhÃ³ Giáº£i ThÃ­ch**: MÃ´ hÃ¬nh "há»™p Ä‘en"

---

## ğŸ”® **HÆ¯á»šNG PHÃT TRIá»‚N TÆ¯Æ NG LAI**

### **Cáº£i Tiáº¿n MÃ´ HÃ¬nh:**
- ğŸ¯ **Attention Mechanism**: ThÃªm cÆ¡ cháº¿ chÃº Ã½
- ğŸ¯ **Bidirectional LSTM**: LSTM hai chiá»u
- ğŸ¯ **Ensemble Methods**: Káº¿t há»£p nhiá»u mÃ´ hÃ¬nh
- ğŸ¯ **Transfer Learning**: Sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n

### **Má»Ÿ Rá»™ng á»¨ng Dá»¥ng:**
- ğŸ“Š **Dá»± ÄoÃ¡n GiÃ¡**: Dá»± Ä‘oÃ¡n giÃ¡ thuÃª xe
- ğŸ“Š **PhÃ¢n TÃ­ch Xu HÆ°á»›ng**: PhÃ¢n tÃ­ch xu hÆ°á»›ng dÃ i háº¡n
- ğŸ“Š **Tá»‘i Æ¯u HÃ³a**: Tá»‘i Æ°u hÃ³a sá»‘ lÆ°á»£ng xe
- ğŸ“Š **Cáº£nh BÃ¡o**: Cáº£nh bÃ¡o nhu cáº§u cao

---

## ğŸ“ **Káº¾T LUáº¬N**

LSTM lÃ  má»™t cÃ´ng nghá»‡ máº¡nh máº½ cho viá»‡c dá»± Ä‘oÃ¡n nhu cáº§u thuÃª xe mÃ¡y:

- ğŸ¯ **Hiá»‡u Quáº£**: Äá»™ chÃ­nh xÃ¡c cao (87%)
- ğŸ¯ **Linh Hoáº¡t**: Xá»­ lÃ½ nhiá»u loáº¡i dá»¯ liá»‡u
- ğŸ¯ **á»”n Äá»‹nh**: Giáº£i quyáº¿t váº¥n Ä‘á» RNN truyá»n thá»‘ng
- ğŸ¯ **Thá»±c Táº¿**: á»¨ng dá»¥ng Ä‘Æ°á»£c trong thá»±c táº¿

MÃ´ hÃ¬nh LSTM Ä‘Ã£ chá»©ng minh kháº£ nÄƒng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nhu cáº§u thuÃª xe mÃ¡y dá»±a trÃªn dá»¯ liá»‡u lá»‹ch sá»­ vÃ  thÃ´ng tin thá»i tiáº¿t, má»Ÿ ra tiá»m nÄƒng lá»›n cho viá»‡c tá»‘i Æ°u hÃ³a hoáº¡t Ä‘á»™ng kinh doanh.
